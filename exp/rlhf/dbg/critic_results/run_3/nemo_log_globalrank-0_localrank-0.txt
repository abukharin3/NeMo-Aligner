[NeMo W 2024-09-16 16:25:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'gpt_ppo_critic': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
      warnings.warn(msg, UserWarning)
    
[NeMo W 2024-09-16 16:25:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
      ret = run_job(
    
[NeMo I 2024-09-16 16:25:46 serve_ppo_critic:48] 
    
    ************** Experiment configuration ***********
[NeMo I 2024-09-16 16:25:46 serve_ppo_critic:49] 
    trainer:
      num_nodes: 1
      devices: 1
      accelerator: gpu
      precision: bf16
      ppo:
        port: 5567
        combine_rm_and_critic_server: true
        max_steps: 10000
        gradient_clip_val: 1.0
        max_queue_delay_microseconds: 2000
        strip_sequence_length_to_multiple: null
        inference_micro_batch_size: ${model.forward_micro_batch_size}
      logger: false
      enable_checkpointing: false
      use_distributed_sampler: false
      max_epochs: null
      max_steps: ${.ppo.max_steps}
    pretrained_checkpoint:
      restore_from_path: /home/abukharin/debug/NeMo-Aligner/data/checkpoints/rm_478m_mcore/megatron_gpt.nemo
    exp_manager:
      explicit_log_dir: /home/abukharin/debug/NeMo-Aligner/exp/rlhf/dbg/critic_results
      exp_dir: null
      name: megatron_gpt_critic
      create_wandb_logger: false
      wandb_logger_kwargs:
        project: nemo_aligner_ppo
        name: gpt3_ppo_2b
      resume_if_exists: true
      resume_ignore_no_checkpoint: true
      create_checkpoint_callback: true
      checkpoint_callback_params:
        monitor: step
        save_top_k: 0
        mode: max
        always_save_nemo: false
        save_nemo_on_train_end: true
        filename: megatron_gpt-{step}
        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
    model:
      micro_batch_size: 1
      global_batch_size: 4
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      virtual_pipeline_model_parallel_size: null
      encoder_seq_length: 4096
      max_position_embeddings: ${.encoder_seq_length}
      num_layers: 12
      hidden_size: 768
      ffn_hidden_size: 3072
      num_attention_heads: 12
      init_method_std: 0.008
      use_scaled_init_method: true
      hidden_dropout: 0.0
      attention_dropout: 0.0
      ffn_dropout: 0.0
      kv_channels: null
      apply_query_key_layer_scaling: true
      normalization: layernorm1p
      layernorm_epsilon: 1.0e-05
      do_layer_norm_weight_decay: false
      make_vocab_size_divisible_by: 128
      pre_process: true
      post_process: true
      persist_layer_norm: true
      bias: false
      activation: squared-relu
      headscale: false
      transformer_block_type: pre_ln
      openai_gelu: false
      normalize_attention_scores: true
      position_embedding_type: rope
      rotary_percentage: 0.5
      attention_type: multihead
      share_embeddings_and_output_weights: false
      tokenizer:
        library: sentencepiece
        type: null
        model: nemo:31c4a6c30c2a49fd88d45de90b27f879_sp_tokenizer_256k.model
        vocab_file: null
        merge_file: null
        tokenizer_model: /lustre/fsw/portfolios/llmservice/projects/llmservice_modelalignment_sft/rlhf/data/sp_model/sp_tokenizer_256k.model
        sentencepiece_legacy: false
      native_amp_init_scale: 4294967296
      native_amp_growth_interval: 1000
      hysteresis: 2
      fp32_residual_connection: false
      fp16_lm_cross_entropy: false
      megatron_amp_O2: true
      grad_allreduce_chunk_size_mb: 125
      grad_div_ar_fusion: true
      gradient_accumulation_fusion: false
      bias_activation_fusion: false
      bias_dropout_add_fusion: false
      masked_softmax_fusion: true
      seed: 1234
      resume_from_checkpoint: null
      use_cpu_initialization: false
      onnx_safe: false
      apex_transformer_log_level: 30
      gradient_as_bucket_view: false
      sync_batch_comm: false
      activations_checkpoint_granularity: null
      activations_checkpoint_method: null
      activations_checkpoint_num_layers: null
      num_micro_batches_with_partial_activation_checkpoints: null
      activations_checkpoint_layers_per_pipeline: null
      sequence_parallel: false
      transformer_engine: false
      fp8: false
      fp8_e4m3: false
      fp8_hybrid: false
      fp8_margin: 0
      fp8_interval: 1
      fp8_amax_history_len: 1
      fp8_amax_compute_algo: most_recent
      reduce_amax: true
      use_emha: false
      optim:
        name: distributed_fused_adam
        lr: 9.0e-06
        weight_decay: 0.1
        betas:
        - 0.9
        - 0.98
        sched:
          name: CosineAnnealing
          warmup_steps: 10
          constant_steps: 1000
          min_lr: 9.0e-07
        bucket_cap_mb: 200
        overlap_grad_sync: false
        contiguous_grad_buffer: true
      data:
        chat: true
        chat_prompt_tokens:
          system_turn_start: <extra_id_0>
          turn_start: <extra_id_1>
          label_start: <extra_id_2>
          end_of_turn: '
    
            '
          end_of_name: '
    
            '
        num_workers: 2
        dataloader_type: single
        train_ds:
          file_path: /dataset/vanilla-guillemot_commercial.shuf.removelong.label.jsonl
          global_batch_size: 128
          micro_batch_size: 2
          shuffle: true
          memmap_workers: null
          max_seq_length: 4096
          min_seq_length: 1
          drop_last: true
          concat_sampling_probabilities: null
          label_key: output
          add_eos: false
          add_sep: false
          add_bos: false
          truncation_field: input
          index_mapping_dir: /indexmap_dir
          prompt_template: '<extra_id_0>System
    
            {system message}
    
            <extra_id_1>User
    
            {turn 1 user message}
    
            <extra_id_1>Assistant
    
            <extra_id_2>{turn 1 assistant label}
    
            {turn 1 assistant message}
    
            <extra_id_1>User
    
            {turn 2 user message}
    
            <extra_id_1>Assistant
    
            <extra_id_2>{turn 2 assistant label}
    
            {turn 2 assistant message}
    
            <extra_id_1>'
          hf_dataset: true
          truncation_method: right
        validation_ds:
          file_path: /dataset/vanilla-guillemot_commercial.shuf.removelong.label.jsonl
          names: null
          global_batch_size: 128
          micro_batch_size: 2
          shuffle: false
          memmap_workers: null
          max_seq_length: 4096
          min_seq_length: 1
          drop_last: false
          label_key: output
          add_eos: false
          add_sep: false
          add_bos: false
          write_predictions_to_file: false
          output_file_path_prefix: null
          truncation_field: input
          index_mapping_dir: /indexmap_dir
          prompt_template: '<extra_id_0>System
    
            {system message}
    
            <extra_id_1>User
    
            {turn 1 user message}
    
            <extra_id_1>Assistant
    
            <extra_id_2>{turn 1 assistant label}
    
            {turn 1 assistant message}
    
            <extra_id_1>User
    
            {turn 2 user message}
    
            <extra_id_1>Assistant
    
            <extra_id_2>{turn 2 assistant label}
    
            {turn 2 assistant message}
    
            <extra_id_1>'
          tokens_to_generate: 32
          hf_dataset: true
          truncation_method: right
          metric:
            name: loss
            average: null
            num_classes: null
        test_ds:
          prompt_template: '<extra_id_0>System
    
            {system message}
    
            <extra_id_1>User
    
            {turn 1 user message}
    
            <extra_id_1>Assistant
    
            <extra_id_2>{turn 1 assistant label}
    
            {turn 1 assistant message}
    
            <extra_id_1>User
    
            {turn 2 user message}
    
            <extra_id_1>Assistant
    
            <extra_id_2>{turn 2 assistant label}
    
            {turn 2 assistant message}
    
            <extra_id_1>'
        data_impl: jsonl
        splits_string: null
        seq_length: 4096
        skip_warmup: true
        reset_position_ids: false
        reset_attention_mask: false
        eod_mask_loss: false
        index_mapping_dir: null
        data_prefix:
          train:
          - /lustre/fsw/portfolios/llmservice/projects/llmservice_modelalignment_sft/rlhf/data/extra_id_prefix_end_with_backslash_n_extra_id_1_jsonl/anthropic_hh_train_comparisons.jsonl
          validation:
          - /lustre/fsw/portfolios/llmservice/projects/llmservice_modelalignment_sft/rlhf/data/extra_id_prefix_end_with_backslash_n_extra_id_1_jsonl/anthropic_hh_val_comparisons.jsonl
          test:
          - /lustre/fsw/portfolios/llmservice/projects/llmservice_modelalignment_sft/rlhf/data/extra_id_prefix_end_with_backslash_n_extra_id_1_jsonl/anthropic_hh_val_comparisons.jsonl
      precision: ${trainer.precision}
      mcore_gpt: true
      answer_only_loss: true
      restore_from_path: /models/unpack_22b_35t_mcore
      save_nemo_on_validation_end: true
      use_flash_attention: null
      pipeline_model_parallel_split_rank: 0
      output_sequence: true
      use_avg_pool: false
      force_head_dtype: float32
      loss_clip_val: 0.2
      offload_adam_states: true
      reward_standardization:
        enable: true
        mean: 0.658
        std: 1.873
      forward_micro_batch_size: ${.micro_batch_size}
      reward_model_type: binary_ranking
      regression:
        num_attributes: 1
        merge_attributes: true
        attribute_weights: null
    
[NeMo W 2024-09-16 16:25:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.
    
[NeMo I 2024-09-16 16:25:46 exp_manager:396] ExpManager schema
[NeMo I 2024-09-16 16:25:46 exp_manager:397] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}}
[NeMo W 2024-09-16 16:25:46 exp_manager:835] Exp_manager is logging to /home/abukharin/debug/NeMo-Aligner/exp/rlhf/dbg/critic_results, but it already exists.
[NeMo W 2024-09-16 16:25:46 exp_manager:757] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/home/abukharin/debug/NeMo-Aligner/exp/rlhf/dbg/critic_results/checkpoints. Training from scratch.
[NeMo I 2024-09-16 16:25:46 exp_manager:455] Experiments will be logged at /home/abukharin/debug/NeMo-Aligner/exp/rlhf/dbg/critic_results
[NeMo I 2024-09-16 16:25:46 exp_manager:983] TensorboardLogger has been set up
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo I 2024-09-16 16:25:48 megatron_init:269] Rank 0 has data parallel group : [0]
[NeMo I 2024-09-16 16:25:48 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]
[NeMo I 2024-09-16 16:25:48 megatron_init:280] All data parallel group ranks with context parallel combined: [[0]]
[NeMo I 2024-09-16 16:25:48 megatron_init:283] Ranks 0 has data parallel rank: 0
[NeMo I 2024-09-16 16:25:48 megatron_init:291] Rank 0 has context parallel group: [0]
[NeMo I 2024-09-16 16:25:48 megatron_init:294] All context parallel group ranks: [[0]]
[NeMo I 2024-09-16 16:25:48 megatron_init:295] Ranks 0 has context parallel rank: 0
[NeMo I 2024-09-16 16:25:48 megatron_init:302] Rank 0 has model parallel group: [0]
[NeMo I 2024-09-16 16:25:48 megatron_init:303] All model parallel group ranks: [[0]]
[NeMo I 2024-09-16 16:25:48 megatron_init:312] Rank 0 has tensor model parallel group: [0]
[NeMo I 2024-09-16 16:25:48 megatron_init:316] All tensor model parallel group ranks: [[0]]
[NeMo I 2024-09-16 16:25:48 megatron_init:317] Rank 0 has tensor model parallel rank: 0
[NeMo I 2024-09-16 16:25:48 megatron_init:337] Rank 0 has pipeline model parallel group: [0]
[NeMo I 2024-09-16 16:25:48 megatron_init:349] Rank 0 has embedding group: [0]
[NeMo I 2024-09-16 16:25:48 megatron_init:355] All pipeline model parallel group ranks: [[0]]
[NeMo I 2024-09-16 16:25:48 megatron_init:356] Rank 0 has pipeline model parallel rank 0
[NeMo I 2024-09-16 16:25:48 megatron_init:357] All embedding group ranks: [[0]]
[NeMo I 2024-09-16 16:25:48 megatron_init:358] Rank 0 has embedding rank: 0
[NeMo I 2024-09-16 16:25:48 num_microbatches_calculator:119] setting number of micro-batches to constant 4
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo I 2024-09-16 16:25:48 tokenizer_utils:193] Getting SentencePiece with model: /tmp/tmp50mpug9q/31c4a6c30c2a49fd88d45de90b27f879_sp_tokenizer_256k.model
[NeMo I 2024-09-16 16:25:48 megatron_base_model:595] Padded vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:1182] The model: MegatronGPTCriticModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:510] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_base_model:568] The model: MegatronGPTCriticModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-09-16 16:25:48 megatron_gpt_model:330] megatron_amp_O2 is enabled but transformer-engine is not.
[NeMo I 2024-09-16 16:25:52 nlp_overrides:1346] Model MegatronGPTCriticModel was successfully restored from /home/abukharin/debug/NeMo-Aligner/data/checkpoints/rm_478m_mcore/megatron_gpt.nemo.
[NeMo I 2024-09-16 16:25:53 megatron_gpt_model:1633] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 4.78e+08. Number of precise model parameters on device: 478189825.
[NeMo I 2024-09-16 16:25:53 modelPT:786] Optimizer config = MegatronDistributedFusedAdam (
    Parameter Group 0
        betas: [0.9, 0.98]
        bias_correction: True
        eps: 1e-08
        is_expert: False
        lr: 9e-06
        weight_decay: 0.1
    
    Parameter Group 1
        betas: [0.9, 0.98]
        bias_correction: True
        eps: 1e-08
        is_expert: False
        lr: 9e-06
        weight_decay: 0.0
    )
[NeMo I 2024-09-16 16:25:53 lr_scheduler:948] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f1413f39db0>" 
    will be used during training (effective maximum steps = 10000) - 
    Parameters : 
    (warmup_steps: 10
    constant_steps: 1000
    min_lr: 9.0e-07
    max_steps: 10000
    )
[NeMo W 2024-09-16 16:25:53 nemo_logging:349] /opt/apex/apex/contrib/optimizers/distributed_fused_adam.py:1488: UserWarning: Only 48.0% of buckets are used. Consider decreasing the bucket_cap_mb argument.
      warnings.warn(
    
[NeMo W 2024-09-16 16:25:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/optimizer.py:316: The lr scheduler dict contains the key(s) ['monitor', 'reduce_on_plateau'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.
    
